<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="统计学习方法," />










<meta name="description" content="决策树(decision tree)的学习包括3个步骤：特征选择，决策树的生成和决策树的修建。 决策树模型与学习决策树模型分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点又分为内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。">
<meta name="keywords" content="统计学习方法">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树">
<meta property="og:url" content="http://yoursite.com/2017/12/25/2017-12-25-decision-tree/index.html">
<meta property="og:site_name" content="嘿喵喵喵喵">
<meta property="og:description" content="决策树(decision tree)的学习包括3个步骤：特征选择，决策树的生成和决策树的修建。 决策树模型与学习决策树模型分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点又分为内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://res.cloudinary.com/kimjin7486/image/upload/v1513867236/1513867015213_itfn93.jpg">
<meta property="og:updated_time" content="2017-12-25T03:52:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="决策树">
<meta name="twitter:description" content="决策树(decision tree)的学习包括3个步骤：特征选择，决策树的生成和决策树的修建。 决策树模型与学习决策树模型分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点又分为内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。">
<meta name="twitter:image" content="http://res.cloudinary.com/kimjin7486/image/upload/v1513867236/1513867015213_itfn93.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/12/25/2017-12-25-decision-tree/"/>





  <title>决策树 | 嘿喵喵喵喵</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">嘿喵喵喵喵</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/25/2017-12-25-decision-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="嘿喵">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嘿喵喵喵喵">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">决策树</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-25T11:23:00+08:00">
                2017-12-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>决策树(decision tree)的学习包括3个步骤：特征选择，决策树的生成和决策树的修建。</p>
<h2 id="决策树模型与学习"><a href="#决策树模型与学习" class="headerlink" title="决策树模型与学习"></a>决策树模型与学习</h2><h3 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h3><p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点又分为内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。</p>
<p><img src="http://res.cloudinary.com/kimjin7486/image/upload/v1513867236/1513867015213_itfn93.jpg" alt="决策树"></p>
<a id="more"></a>
<h3 id="决策树与if-then规则"><a href="#决策树与if-then规则" class="headerlink" title="决策树与if-then规则"></a>决策树与if-then规则</h3><p>将决策树转换成if-then规则的过程：由决策树的根节点到叶结点的每一条路径构建一条规则：路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。</p>
<h3 id="决策树与条件概率分布"><a href="#决策树与条件概率分布" class="headerlink" title="决策树与条件概率分布"></a>决策树与条件概率分布</h3><p>决策树也能表示给定特征条件下类的条件概率分布。</p>
<h3 id="决策树学习"><a href="#决策树学习" class="headerlink" title="决策树学习"></a>决策树学习</h3><p>决策树学习的损失函数通常是正则化的极大似然函数。学习的策略就是以损失函数为目标函数的最小化。</p>
<p>因为从所有可能的决策树中选取最优决策树是NP完全问题（多项式复杂度非确定性问题），决策树学习算法通常采用启发式方法，近似求解得到次最优的决策树。</p>
<p>决策树的学习算法包含特征选择，决策树生成与决策树剪枝的过程。</p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><h3 id="特征选择问题"><a href="#特征选择问题" class="headerlink" title="特征选择问题"></a>特征选择问题</h3><p>特征选择的准则是信息增益或者信息增益比</p>
<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>在信息论与概率统计中，熵(entropy)表示随机变量不确定性的度量，假设X是一个取有限个值的离散随机变量，其概率分布为</p>
<p>$$<br>P(X=x_i)=p_i, i=1,2,\cdots,n<br>$$</p>
<p>那么X的熵定义为:</p>
<p>$$<br>H(X)=-\sum_{i=1}^n p_ilogp_i<br>$$</p>
<p>因为熵定义只依赖于X的分布，与X的取值无关，所以通常也记作：</p>
<p>$$<br>H(p)=\sum_{i=1}^np_ilogp_i<br>$$</p>
<p>对数以2为底时，熵的单位为比特(bit)，以自然对数e为底时，单位为纳特(nat)</p>
<p>熵越大，随机变量的不确定性就越大。</p>
<p>设有随机变量(X, Y),其联合概率分布为:</p>
<p>$$<br>P(X=x_i, Y=y_i)=p_{ij}, i=1,2,\cdots,n;j=1,2,\cdots,m<br>$$</p>
<p>条件熵H(Y|X)表示在已知随机变量X的条件下，随机变量Y的不确定性。随机变量X在给定条件下随机变量Y的条件熵定义为，X给定条件下Y的条件概率分布的熵对X的数学期望：</p>
<p>$$<br>H(Y|X) = \sum_{i=1}^np_iH(Y|X=x_i)<br>$$</p>
<p>熵和条件熵中的概率由数据统计（极大似然估计）得到时，此时的熵和条件熵称为经验熵(empirical entropy)和经验条件熵(empirical conditional entropy)。</p>
<p>信息增益(information gain)表示得知特征X的信息而使得Y的信息的不确定性减少的程度。（按我的理解就是说知道了特征X之后，就可以更加确定Y了）</p>
<p>特征A对训练集D的信息增益g(D, A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差(也称为互信息(mutual information)，即</p>
<p>$$<br>g(D, A) = H(D) - H(D|A)<br>$$</p>
<hr>
<p>设训练集为D，|D|表示样本容量（样本个数）。设有K个类$C_k, k=1,2,\cdots,K$，$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K|C_k|=|D|$.设特征A有n个不同的取值${a_1, a_2,\cdots,a_n}$，特征A的取值将D划分为n个子集$D_1,D_2,\cdots,D_n$,$|D_i|$为$D_i$的样本个数,$\sum_{i=1}^n|D_i|=|D|$.记子集$D_i$属于类$C_k$的样本集合为$D_{ik}$</p>
<p>输入：训练集D和特征A</p>
<p>输出：特征A对训练集D的信息增益g(D, A)</p>
<ol>
<li>计算数据集D的经验熵H(D)</li>
</ol>
<p>$$<br>H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}<br>$$</p>
<ol>
<li>计算特征A对数据集D的经验条件熵</li>
</ol>
<p>$$<br>H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}<br>$$</p>
<ol>
<li>计算信息增益</li>
</ol>
<p>$$<br>g(D, A) = H(D) - H(D|A)<br>$$</p>
<hr>
<p>信息增益越大，则不确定性越低。所以选择信息增益越大的值作为最优特征。</p>
<h3 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h3><p>在训练数据集的经验熵较大时，信息增益也会偏大。使用信息增益比可以校正此问题</p>
<p>特征A对训练集D的信息增益比$g_R(D, A)$定义为信息增益比与经验熵之比:</p>
<p>$$<br>g_R(D, A) = \frac{g(D, A)}{H(D)}<br>$$</p>
<h2 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h2><h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h3><hr>
<p>ID3算法</p>
<p>输入：训练集D，特征集A，阈值$\epsilon$</p>
<p>输出：决策树T</p>
<ol>
<li><p>若D中所有实例属于同一类$C_k$, 则T为单结点树，并将类$C_k$作为该结点的类标记，返回T</p>
</li>
<li><p>若$A=\emptyset$，则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T</p>
</li>
<li>否则，根据信息增益算法计算A中各个特征对D的信息增益，选择增益最大的特征$A_g$</li>
<li>如果$A_g$小于阈值$\epsilon$,则置T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T</li>
<li>否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将D分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点。由结点及其子结点构成树T，返回T</li>
<li>对第i个子结点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归调用1-5,得到子树$T_i$,返回T</li>
</ol>
<hr>
<p>ID3算法只生成了树，所以容易产生过拟合。</p>
<h3 id="C4-5的生成算法"><a href="#C4-5的生成算法" class="headerlink" title="C4.5的生成算法"></a>C4.5的生成算法</h3><p>C4.5与ID3的对比在于，C4.5使用的是信息增益比来选择特征:</p>
<hr>
<p>输入：训练集D，特征集A，阈值$\epsilon$</p>
<p>输出：决策树T</p>
<ol>
<li><p>若D中所有实例属于同一类$C_k$, 则T为单结点树，并将类$C_k$作为该结点的类标记，返回T</p>
</li>
<li><p>若$A=\emptyset$，则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T</p>
</li>
<li>否则，根据信息增益比算法计算A中各个特征对D的信息增益比，选择增益比最大的特征$A_g$</li>
<li>如果$A_g$小于阈值$\epsilon$,则置T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T</li>
<li>否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将D分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点。由结点及其子结点构成树T，返回T</li>
<li>对第i个子结点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归调用1-5,得到子树$T_i$,返回T</li>
</ol>
<hr>
<h2 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h2><p>在决策树学习中将已生成的树进行简化的过程称为剪枝(pruning)。即从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点。</p>
<p>决策树剪枝一般通过极小化决策树整体的损失函数或代价函数来实现。设树T的叶结点个数为|T|,t是树T的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个。$H_t(T)$为叶结点t上的经验熵,$\alpha \ge 0$为参数，则决策树学习的损失函数可以定义为:</p>
<p>$$<br>C_\alpha(T)=C(T)+\alpha|T|\<br>=\sum_{t=1}^{|T|}N_tH_t(T) + \alpha |T|\<br>=-\sum_{t=1}^{|T|}\sum_k^KN_{tk}log\frac{N_{tk}}{N_t}<br>$$</p>
<p>$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度。$|T|$表示模型复杂度。</p>
<p>决策树的生成只考虑通过信息增益来对训练集更好的拟合，而决策树剪枝则通过优化损失函数来减小模型复杂度。决策树生成学习局部模型，决策树剪枝学习整体的模型。</p>
<hr>
<p>剪枝算法</p>
<p>输入：生成算法产生的整个树T，参数$\alpha$</p>
<p>输出：修剪后的子树$T_\alpha$</p>
<ol>
<li>计算每个结点的经验熵</li>
<li>递归地从树的叶结点向上回缩</li>
</ol>
<p>设一组叶结点会所到其父结点之前与之后的整体树分别为$T_B$和$T_A$，其对应的损失函数值分别是$C_\alpha(T_B)$和$C_\alpha(T_A)$，如果</p>
<p>$$<br>C_\alpha(T_A) \le C_\alpha(T_B)<br>$$</p>
<p>则进行剪枝，即将父结点变为新的叶结点。</p>
<ol>
<li>返回2，直至不能继续为止，得到损失函数最小的子树$T_\alpha$</li>
</ol>
<hr>
<h2 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h2><p>CART算法指的是分类与回归树(classification and regression tree, CART).</p>
<p>CART算法主要包含以下两个步骤：</p>
<ol>
<li>决策树生成：基于训练集生成决策树，生成的决策树要尽量大</li>
<li>决策树剪枝：用验证集对已生成的树进行剪枝并选择最优子树。</li>
</ol>
<h3 id="CART生成"><a href="#CART生成" class="headerlink" title="CART生成"></a>CART生成</h3><p>决策树的生成就是递归构建二叉决策树的过程。对于回归树使用平方误差最小化准则，对分类树使用基尼指数(Gini index)最小化准则。</p>
<ul>
<li>回归树的生成</li>
</ul>
<p>回归树对应着输入空间的划分以及在划分的单元上的输出值。假设已经将输入空间划分为M个单元$R_1, R_2,\cdots,R_M$，并且在每个单元$R_m$有固定的输出值$c_m$,则回归树模型可表示为:</p>
<p>$$<br>f(x) = \sum_{m=1}^Mc_mI(x\in R_m)<br>$$</p>
<p>单元$R_m$上的$c_m$的最优值$\hat{c_m}$是$R_m$上所有输入实例$x_i$对应的输出$y_i$的均值，即</p>
<p>$$<br>\hat{c_m}=ave(y_i|x_i\in R_m)<br>$$</p>
<p>一般采用启发式的方法对空间进行划分：选择第j个变量$x^{(j)}$以及它所取的值s作为切分变量和切分点，并定义两个区域：</p>
<p>$$<br>R_1(j,s) = {x|x^{(j)} \le s}\<br>R_2(j,s) = {x|x^{(j)} &gt; s}<br>$$</p>
<p>然后寻找最优切分变量j和最优切分点s，即求解:</p>
<p>$$<br>min_{j,s}[min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]<br>$$</p>
<p>对固定输入变量j可以找到最优切分点s</p>
<p>$$<br>\hat{c_1} = ave(y_i|x_i\in R_1(j,s))\<br>\hat{c_2} = ave(y_i|x_i\in R_2(j,s))<br>$$</p>
<p>遍历所有输入变量，找到最优切分变量j，构成(j,s)对。依次将空间划分为两个区域。重复直到满足停止条件。这样生成的树通常称为最小二乘回归树(least squares regression tree)</p>
<hr>
<p>最小二乘回归树生成算法</p>
<p>输入：训练集D</p>
<p>输出：回归树f(x)</p>
<p>在训练集的输入空间中，递归将每个区域划分成两个子区域并决定子区域的输出值，构建二叉决策树：</p>
<ol>
<li>选择最优切分变量j与切分点s，求解：</li>
</ol>
<p>$$<br>min_{j,s}[min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]<br>$$</p>
<p>遍历变量j，对固定的切分变量j扫描切分点s，选择使上式达到最小值的对(j, s)</p>
<ol>
<li>用选定的对(j,s)划分区域并决定相应的输出值：</li>
</ol>
<p>$$<br>R_1(j,s) = {x|x^{(j)} \le s}\<br>R_2(j,s) = {x|x^{(j)} &gt; s}\<br>\hat{c_m} = \frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i<br>$$</p>
<ol>
<li>继续对两个子区域调用上述步骤，直到满足停止条件</li>
<li>将输入空间划分为M个区域$R_1, R_2,\cdots, R_M$，生成决策树：</li>
</ol>
<p>$$<br>f(x)=\sum_{m=1}^M\hat{c_m}I(x\in R_m)</p>
<h2 id=""><a href="#" class="headerlink" title="$$"></a>$$</h2><ul>
<li>分类树的生成</li>
</ul>
<p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点</p>
<p>分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$,则概率分布的基尼指数为:</p>
<p>$$<br>Gini(p) = \sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2<br>$$</p>
<p>对于二类分类问题，概率分布的基尼指数为:</p>
<p>$$<br>Gini(p)=2p(1-p)<br>$$</p>
<p>对于给定的样本集合D，基尼指数为:</p>
<p>$$<br>Gini(D) = 1 - \sum_{k=1}^K(\frac{|C_k|}{|D|})^2<br>$$</p>
<p>如果样本集合D根据特征A可以分割为$D_1$和$D_2$两部分，则在特征A的条件下，集合D的基尼指数为:</p>
<p>$$<br>Ginig(D, A) = \frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)<br>$$</p>
<p>基尼指数表示集合D的不确定性（与熵相似）</p>
<hr>
<p>CART生成算法</p>
<p>输入：训练集D，停止计算的条件</p>
<p>输出: CART决策树</p>
<p>根据训练集，从根结点开始，递归对每个结点进行以下操作，构建二叉决策树：</p>
<ol>
<li>设结点训练集为D，计算现有特征对该数据集的基尼指数。此时，对每个特征A,对其可能取的每个值a，根据样本点对A=a的的测试分割为$D_1$和$D_2$两部分，计算A=a时的基尼指数</li>
<li>在所有可能的特征A及它们所有可能的切分点a中，选择基尼指数最小的特征及其对应的切分点作为最优选。依据最优选择，从现结点生成两个子结点，将训练集依据特征分配到两个子结点中去</li>
<li>对两个子结点递归调用上述步骤，直到满足停止条件</li>
<li>生成CART决策树</li>
</ol>
<p>停止条件一般是指结点中样本个数小于预定阈值或基尼指数小于阈值。</p>
<hr>
<h3 id="CART剪枝"><a href="#CART剪枝" class="headerlink" title="CART剪枝"></a>CART剪枝</h3><p>剪枝算法：从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列${T_0,T_1,\cdots, T_n}$;然后通过交叉验证法在独立的验证集上进行测试，选择最优子树。</p>
<ol>
<li>剪枝，形成一个子树序列</li>
</ol>
<p>从整体树$T_0$开始剪枝，对其任意内部结点t，以t为单结点树的损失函数为:</p>
<p>$$<br>C_\alpha(t) = C(t) + \alpha<br>$$</p>
<p>以t为根结点的子树$T_t$损失函数为：</p>
<p>$$<br>C_\alpha(T_t) = C(T_t) + \alpha|T_t|<br>$$</p>
<p>当$\alpha = \frac{C(t)-C(T_t)}{|T_t|-1}$时，$T_t$跟t有相同的损失函数，此时t的结点更少，所以更可取，进行剪枝。</p>
<p>所以对$T_0$内每一个内部结点t，计算$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$，它表示剪枝后整体损失函数减少的成都。在$T_0$中减去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\alpha_1$，$T_1$作为区间$[\alpha_1,\alpha_2)`的最优子树。</p>
<p>重复此过程，直到得到根结点。</p>
<ol>
<li>通过验证集选取最优子树</li>
</ol>
<p>通过平方误差或者基尼指数测试子树序列。</p>
<hr>
<p>CART剪枝算法</p>
<p>输入：CART生成的决策树</p>
<p>输出：最优决策树$T_\alpha$</p>
<ol>
<li>设$k=0, T=T_0$</li>
<li>设$\alpha= +\infty$</li>
<li>自下而上对各内部结点t计算$C(T_t),|T_t|$以及</li>
</ol>
<p>$$<br>g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}\<br>\alpha = min(\alpha, g(t))<br>$$<br>这里$T_t$表示以t为根结点的子树，$C(T_t)$是对训练集的预测误差，$|T_t|$是$T_t$叶结点个数</p>
<ol>
<li>自上而下访问内部结点，如果有g(t)=$\alpha$，进行剪枝，并对叶结点t以多数表决决定其类，得到树T</li>
<li>设k=k+1,$\alpha_k=\alpha, T_k=T$</li>
<li>如果T不是由根结点单独构成的树，回到4</li>
<li>使用交叉验证法选取最优子树</li>
</ol>
<hr>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/统计学习方法/" rel="tag"># 统计学习方法</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/12/21/2017-12-21-Naive-Bayes/" rel="next" title="朴素贝叶斯">
                <i class="fa fa-chevron-left"></i> 朴素贝叶斯
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/12/27/2017-12-17-Logistics/" rel="prev" title="Logistic回归模型与最大熵模型">
                Logistic回归模型与最大熵模型 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="嘿喵" />
            
              <p class="site-author-name" itemprop="name">嘿喵</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/KimJin77" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树模型与学习"><span class="nav-number">1.</span> <span class="nav-text"><a href="#&#x51B3;&#x7B56;&#x6811;&#x6A21;&#x578B;&#x4E0E;&#x5B66;&#x4E60;" class="headerlink" title="&#x51B3;&#x7B56;&#x6811;&#x6A21;&#x578B;&#x4E0E;&#x5B66;&#x4E60;"></a>&#x51B3;&#x7B56;&#x6811;&#x6A21;&#x578B;&#x4E0E;&#x5B66;&#x4E60;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树模型"><span class="nav-number">1.1.</span> <span class="nav-text"><a href="#&#x51B3;&#x7B56;&#x6811;&#x6A21;&#x578B;" class="headerlink" title="&#x51B3;&#x7B56;&#x6811;&#x6A21;&#x578B;"></a>&#x51B3;&#x7B56;&#x6811;&#x6A21;&#x578B;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树与if-then规则"><span class="nav-number">1.2.</span> <span class="nav-text"><a href="#&#x51B3;&#x7B56;&#x6811;&#x4E0E;if-then&#x89C4;&#x5219;" class="headerlink" title="&#x51B3;&#x7B56;&#x6811;&#x4E0E;if-then&#x89C4;&#x5219;"></a>&#x51B3;&#x7B56;&#x6811;&#x4E0E;if-then&#x89C4;&#x5219;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树与条件概率分布"><span class="nav-number">1.3.</span> <span class="nav-text"><a href="#&#x51B3;&#x7B56;&#x6811;&#x4E0E;&#x6761;&#x4EF6;&#x6982;&#x7387;&#x5206;&#x5E03;" class="headerlink" title="&#x51B3;&#x7B56;&#x6811;&#x4E0E;&#x6761;&#x4EF6;&#x6982;&#x7387;&#x5206;&#x5E03;"></a>&#x51B3;&#x7B56;&#x6811;&#x4E0E;&#x6761;&#x4EF6;&#x6982;&#x7387;&#x5206;&#x5E03;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树学习"><span class="nav-number">1.4.</span> <span class="nav-text"><a href="#&#x51B3;&#x7B56;&#x6811;&#x5B66;&#x4E60;" class="headerlink" title="&#x51B3;&#x7B56;&#x6811;&#x5B66;&#x4E60;"></a>&#x51B3;&#x7B56;&#x6811;&#x5B66;&#x4E60;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征选择"><span class="nav-number">2.</span> <span class="nav-text"><a href="#&#x7279;&#x5F81;&#x9009;&#x62E9;" class="headerlink" title="&#x7279;&#x5F81;&#x9009;&#x62E9;"></a>&#x7279;&#x5F81;&#x9009;&#x62E9;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#特征选择问题"><span class="nav-number">2.1.</span> <span class="nav-text"><a href="#&#x7279;&#x5F81;&#x9009;&#x62E9;&#x95EE;&#x9898;" class="headerlink" title="&#x7279;&#x5F81;&#x9009;&#x62E9;&#x95EE;&#x9898;"></a>&#x7279;&#x5F81;&#x9009;&#x62E9;&#x95EE;&#x9898;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#信息增益"><span class="nav-number">2.2.</span> <span class="nav-text"><a href="#&#x4FE1;&#x606F;&#x589E;&#x76CA;" class="headerlink" title="&#x4FE1;&#x606F;&#x589E;&#x76CA;"></a>&#x4FE1;&#x606F;&#x589E;&#x76CA;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#信息增益比"><span class="nav-number">2.3.</span> <span class="nav-text"><a href="#&#x4FE1;&#x606F;&#x589E;&#x76CA;&#x6BD4;" class="headerlink" title="&#x4FE1;&#x606F;&#x589E;&#x76CA;&#x6BD4;"></a>&#x4FE1;&#x606F;&#x589E;&#x76CA;&#x6BD4;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树的生成"><span class="nav-number">3.</span> <span class="nav-text"><a href="#&#x51B3;&#x7B56;&#x6811;&#x7684;&#x751F;&#x6210;" class="headerlink" title="&#x51B3;&#x7B56;&#x6811;&#x7684;&#x751F;&#x6210;"></a>&#x51B3;&#x7B56;&#x6811;&#x7684;&#x751F;&#x6210;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ID3算法"><span class="nav-number">3.1.</span> <span class="nav-text"><a href="#ID3&#x7B97;&#x6CD5;" class="headerlink" title="ID3&#x7B97;&#x6CD5;"></a>ID3&#x7B97;&#x6CD5;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C4-5的生成算法"><span class="nav-number">3.2.</span> <span class="nav-text"><a href="#C4-5&#x7684;&#x751F;&#x6210;&#x7B97;&#x6CD5;" class="headerlink" title="C4.5&#x7684;&#x751F;&#x6210;&#x7B97;&#x6CD5;"></a>C4.5&#x7684;&#x751F;&#x6210;&#x7B97;&#x6CD5;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树的剪枝"><span class="nav-number">4.</span> <span class="nav-text"><a href="#&#x51B3;&#x7B56;&#x6811;&#x7684;&#x526A;&#x679D;" class="headerlink" title="&#x51B3;&#x7B56;&#x6811;&#x7684;&#x526A;&#x679D;"></a>&#x51B3;&#x7B56;&#x6811;&#x7684;&#x526A;&#x679D;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CART算法"><span class="nav-number">5.</span> <span class="nav-text"><a href="#CART&#x7B97;&#x6CD5;" class="headerlink" title="CART&#x7B97;&#x6CD5;"></a>CART&#x7B97;&#x6CD5;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CART生成"><span class="nav-number">5.1.</span> <span class="nav-text"><a href="#CART&#x751F;&#x6210;" class="headerlink" title="CART&#x751F;&#x6210;"></a>CART&#x751F;&#x6210;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-number">6.</span> <span class="nav-text"><a href="#" class="headerlink" title="$$"></a>$$</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CART剪枝"><span class="nav-number">6.1.</span> <span class="nav-text"><a href="#CART&#x526A;&#x679D;" class="headerlink" title="CART&#x526A;&#x679D;"></a>CART&#x526A;&#x679D;</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">嘿喵</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
